{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "db2cad75",
   "metadata": {},
   "source": [
    "# Pneumonia detection using chest x-ray ##"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "578ecced",
   "metadata": {},
   "source": [
    "## Import useful librairies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fc48e9b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from glob import glob\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow import keras\n",
    "from keras.layers import Input, Lambda, Dense, Flatten\n",
    "from keras.models import Model\n",
    "from keras.applications.vgg16 import VGG16\n",
    "from keras.applications.vgg16 import preprocess_input\n",
    "from keras.preprocessing import image\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import Sequential\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5edd62ab",
   "metadata": {},
   "source": [
    "## Load images ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4535ccb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "size_image = [224, 224]\n",
    "train_path = 'chest_xray/chest_xray/TRAIN'\n",
    "val_path = 'chest_xray/chest_xray/TEST'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2ff33a8",
   "metadata": {},
   "source": [
    "## What is VGG16 model ? ##\n",
    "A convolutional neural network is also known as a ConvNet, which is a kind of artificial neural network. A convolutional neural network has an input layer, an output layer, and various hidden layers. VGG16 is a type of CNN (Convolutional Neural Network) that is considered to be one of the best computer vision models to date. The creators of this model evaluated the networks and increased the depth using an architecture with very small (3 × 3) convolution filters, which showed a significant improvement on the prior-art configurations. They pushed the depth to 16–19 weight layers making it approx — 138 trainable parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "46ee88e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "vgg = VGG16(input_shape = size_image + [3], weights = 'imagenet', include_top=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "38720208",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in vgg.layers:\n",
    "    i.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b12b8e5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "folder = glob('chest_xray/chest_xray/TRAIN/*')\n",
    "x = Flatten()(vgg.output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d572f27",
   "metadata": {},
   "source": [
    "Here, we are using Dense because it's the task of predicting a label for each pixel of the image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8f29387e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 224, 224, 3)]     0         \n",
      "                                                                 \n",
      " block1_conv1 (Conv2D)       (None, 224, 224, 64)      1792      \n",
      "                                                                 \n",
      " block1_conv2 (Conv2D)       (None, 224, 224, 64)      36928     \n",
      "                                                                 \n",
      " block1_pool (MaxPooling2D)  (None, 112, 112, 64)      0         \n",
      "                                                                 \n",
      " block2_conv1 (Conv2D)       (None, 112, 112, 128)     73856     \n",
      "                                                                 \n",
      " block2_conv2 (Conv2D)       (None, 112, 112, 128)     147584    \n",
      "                                                                 \n",
      " block2_pool (MaxPooling2D)  (None, 56, 56, 128)       0         \n",
      "                                                                 \n",
      " block3_conv1 (Conv2D)       (None, 56, 56, 256)       295168    \n",
      "                                                                 \n",
      " block3_conv2 (Conv2D)       (None, 56, 56, 256)       590080    \n",
      "                                                                 \n",
      " block3_conv3 (Conv2D)       (None, 56, 56, 256)       590080    \n",
      "                                                                 \n",
      " block3_pool (MaxPooling2D)  (None, 28, 28, 256)       0         \n",
      "                                                                 \n",
      " block4_conv1 (Conv2D)       (None, 28, 28, 512)       1180160   \n",
      "                                                                 \n",
      " block4_conv2 (Conv2D)       (None, 28, 28, 512)       2359808   \n",
      "                                                                 \n",
      " block4_conv3 (Conv2D)       (None, 28, 28, 512)       2359808   \n",
      "                                                                 \n",
      " block4_pool (MaxPooling2D)  (None, 14, 14, 512)       0         \n",
      "                                                                 \n",
      " block5_conv1 (Conv2D)       (None, 14, 14, 512)       2359808   \n",
      "                                                                 \n",
      " block5_conv2 (Conv2D)       (None, 14, 14, 512)       2359808   \n",
      "                                                                 \n",
      " block5_conv3 (Conv2D)       (None, 14, 14, 512)       2359808   \n",
      "                                                                 \n",
      " block5_pool (MaxPooling2D)  (None, 7, 7, 512)         0         \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 25088)             0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 2)                 50178     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 14,764,866\n",
      "Trainable params: 50,178\n",
      "Non-trainable params: 14,714,688\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# first we define dense to predict each pixel\n",
    "pred = Dense(len(folder), activation='softmax')(x)\n",
    "\n",
    "# then we create the CNN model that we want to generate\n",
    "model = Model(inputs = vgg.input, outputs = pred)\n",
    "\n",
    "# finally we look at the summary of the model (architecture)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c3aed5b",
   "metadata": {},
   "source": [
    "When our model is created, we can compile it :\n",
    "- Categorical cross-entropy is used as a loss function for multi-class classification model where there are two or more output labels. The output label is assigned one-hot category encoding value in form of 0s and 1. The output label, if present in integer form, is converted into categorical encoding using keras.utils to_categorical method.\n",
    "- Adam, derived from Adaptive Moment Estimation, is an optimization algorithm. The Adam optimizer makes use of a combination of ideas from other optimizers. Similar to the momentum optimizer, Adam makes use of an exponentially decaying average of past gradients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1073bbb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b61180a",
   "metadata": {},
   "source": [
    "## Data generator ##\n",
    "Keras ImageDataGenerator is used for getting the input of the original data and further, it makes the transformation of this data on a random basis and gives the output resultant containing only the data that is newly transformed. It does not add the data. Keras image data generator class is also used to carry out data augmentation where we aim to gain the overall increment in the generalization of the model. Operations such as rotations, translations, shearin, scale changes, and horizontal flips are carried out randomly in data augmentation using an image data generator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c4181b7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 5216 images belonging to 2 classes.\n",
      "Found 624 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "train_datagen = ImageDataGenerator(rescale = 1./255,shear_range = 0.2,zoom_range = 0.2,horizontal_flip = True)\n",
    "test_datagen = ImageDataGenerator(rescale = 1./255)\n",
    "training_set = train_datagen.flow_from_directory('chest_xray/chest_xray/TRAIN',target_size = (224, 224),batch_size = 10,class_mode = 'categorical')\n",
    "test_set = test_datagen.flow_from_directory('chest_xray/chest_xray/TEST',target_size = (224, 224),batch_size = 10,class_mode = 'categorical')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1473917e",
   "metadata": {},
   "source": [
    "## Fit the model ##\n",
    "This step is very long because it has to fit our model on each data, it means on each picture of our folder chest_xray."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3ada4fc8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "522/522 [==============================] - 1267s 2s/step - loss: 0.1996 - accuracy: 0.9300 - val_loss: 0.3292 - val_accuracy: 0.9135\n"
     ]
    }
   ],
   "source": [
    "model_fit = model.fit_generator(training_set,validation_data = test_set,epochs = 1,steps_per_epoch = len(training_set),validation_steps = len(test_set))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b50c1ebd",
   "metadata": {},
   "source": [
    "## Save the model ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "13514563",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('model_test.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b08784a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "50d23e4e8fe43b19e3544ed3c316760529da797f5b6bf0bf0b36c6e437470344"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
